# backend/pipeline_manager.py
"""æµæ°´çº¿ç®¡ç†å™¨ - ç”¨äºç®¡ç†é¢„é…ç½®çš„ Git æ„å»ºæµæ°´çº¿ï¼ˆåŸºäºæ•°æ®åº“ï¼‰"""
import json
import uuid
import hmac
import hashlib
import threading
from datetime import datetime
from typing import Optional, Dict, List
from sqlalchemy.orm import Session
from backend.database import get_db_session, init_db
from backend.models import Pipeline, PipelineTaskHistory

# ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
try:
    init_db()
except:
    pass


class PipelineManager:
    """æµæ°´çº¿ç®¡ç†å™¨ï¼ˆåŸºäºæ•°æ®åº“ï¼‰"""

    def __init__(self):
        self.lock = threading.RLock()
        # å­˜å‚¨æ¯ä¸ªæµæ°´çº¿æœ€åä¸€æ¬¡è§¦å‘çš„é…ç½®ä¿¡æ¯ï¼ˆç”¨äºé˜²æŠ–æ£€æŸ¥ï¼‰
        # æ ¼å¼: {pipeline_id: {"config_hash": str, "timestamp": datetime}}
        self._last_trigger_configs = {}

    def _safe_get_json_field(self, pipeline, field_name, default_value):
        """å®‰å…¨åœ°è·å– JSON å­—æ®µï¼Œå¦‚æœè§£ç å¤±è´¥è¿”å›é»˜è®¤å€¼"""
        try:
            value = getattr(pipeline, field_name, None)
            if value is None:
                return default_value
            # å¦‚æœå·²ç»æ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
            if isinstance(value, (dict, list)):
                return value
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æï¼ˆè™½ç„¶ SQLAlchemy åº”è¯¥å·²ç»è§£æäº†ï¼‰
            if isinstance(value, str):
                import json

                return json.loads(value) if value else default_value
            return value
        except Exception as e:
            print(f"âš ï¸ è·å–å­—æ®µ {field_name} å¤±è´¥: {e}")
            return default_value

    def _to_dict(self, pipeline: Pipeline) -> Dict:
        """å°†æ•°æ®åº“æ¨¡å‹è½¬æ¢ä¸ºå­—å…¸"""
        if not pipeline:
            return None

        try:
            return {
                "pipeline_id": pipeline.pipeline_id,
                "name": pipeline.name,
                "description": pipeline.description,
                "enabled": pipeline.enabled,
                "git_url": pipeline.git_url,
                "branch": pipeline.branch,
                "sub_path": pipeline.sub_path,
                "project_type": pipeline.project_type,
                "template": pipeline.template,
                "image_name": pipeline.image_name,
                "tag": pipeline.tag,
                "push": pipeline.push,
                "push_registry": pipeline.push_registry,
                "template_params": self._safe_get_json_field(
                    pipeline, "template_params", {}
                ),
                "use_project_dockerfile": pipeline.use_project_dockerfile,
                "dockerfile_name": pipeline.dockerfile_name,
                "webhook_token": pipeline.webhook_token,
                "webhook_secret": pipeline.webhook_secret,
                "webhook_branch_filter": pipeline.webhook_branch_filter,
                "webhook_use_push_branch": pipeline.webhook_use_push_branch,
                "webhook_allowed_branches": self._safe_get_json_field(
                    pipeline, "webhook_allowed_branches", []
                ),
                "branch_tag_mapping": self._safe_get_json_field(
                    pipeline, "branch_tag_mapping", {}
                ),
                "source_id": pipeline.source_id,
                "selected_services": self._safe_get_json_field(
                    pipeline, "selected_services", []
                ),
                "service_push_config": self._safe_get_json_field(
                    pipeline, "service_push_config", {}
                ),
                "service_template_params": self._safe_get_json_field(
                    pipeline, "service_template_params", {}
                ),
                "push_mode": pipeline.push_mode,
                "resource_package_configs": self._safe_get_json_field(
                    pipeline, "resource_package_configs", []
                ),
                "cron_expression": pipeline.cron_expression,
                "next_run_time": (
                    pipeline.next_run_time.isoformat()
                    if pipeline.next_run_time
                    else None
                ),
                "post_build_webhooks": self._safe_get_json_field(
                    pipeline, "post_build_webhooks", []
                ),
                "current_task_id": pipeline.current_task_id,
                "task_queue": self._safe_get_json_field(pipeline, "task_queue", []),
                "created_at": (
                    pipeline.created_at.isoformat() if pipeline.created_at else None
                ),
                "updated_at": (
                    pipeline.updated_at.isoformat() if pipeline.updated_at else None
                ),
                "last_triggered_at": (
                    pipeline.last_triggered_at.isoformat()
                    if pipeline.last_triggered_at
                    else None
                ),
                "trigger_count": pipeline.trigger_count,
            }
        except Exception as e:
            print(
                f"âš ï¸ è½¬æ¢æµæ°´çº¿ {pipeline.pipeline_id if pipeline else 'None'} ä¸ºå­—å…¸å¤±è´¥: {e}"
            )
            import traceback

            traceback.print_exc()
            # è¿”å›åŸºæœ¬ä¿¡æ¯ï¼Œé¿å…å®Œå…¨å¤±è´¥
            return {
                "pipeline_id": getattr(pipeline, "pipeline_id", None),
                "name": getattr(pipeline, "name", "Unknown"),
                "enabled": getattr(pipeline, "enabled", False),
                "error": f"è½¬æ¢å¤±è´¥: {str(e)}",
            }

    def create_pipeline(
        self,
        name: str,
        git_url: str,
        branch: str = None,
        project_type: str = "jar",
        template: str = None,
        image_name: str = None,
        tag: str = "latest",
        push: bool = False,
        push_registry: str = None,
        template_params: dict = None,
        sub_path: str = None,
        use_project_dockerfile: bool = True,
        dockerfile_name: str = "Dockerfile",
        webhook_secret: str = None,
        webhook_token: str = None,
        enabled: bool = True,
        description: str = "",
        cron_expression: str = None,
        webhook_branch_filter: bool = False,
        webhook_use_push_branch: bool = True,
        webhook_allowed_branches: list = None,
        branch_tag_mapping: dict = None,
        source_id: str = None,
        selected_services: list = None,
        service_push_config: dict = None,
        service_template_params: dict = None,
        push_mode: str = "multi",
        resource_package_configs: list = None,
        post_build_webhooks: list = None,
    ) -> str:
        """åˆ›å»ºæµæ°´çº¿é…ç½®"""
        pipeline_id = str(uuid.uuid4())

        # ç”Ÿæˆ Webhook Token
        if not webhook_token:
            webhook_token = str(uuid.uuid4())

        # æ£€æŸ¥ token æ˜¯å¦å·²è¢«ä½¿ç”¨
        db = get_db_session()
        try:
            existing = (
                db.query(Pipeline)
                .filter(Pipeline.webhook_token == webhook_token)
                .first()
            )
            if existing:
                raise ValueError(f"Webhook Token '{webhook_token}' å·²è¢«å…¶ä»–æµæ°´çº¿ä½¿ç”¨")

            # å¦‚æœæ²¡æœ‰æä¾› webhook_secretï¼Œç”Ÿæˆä¸€ä¸ª
            if not webhook_secret:
                webhook_secret = str(uuid.uuid4())

            pipeline = Pipeline(
                pipeline_id=pipeline_id,
                name=name,
                description=description,
                enabled=enabled,
                git_url=git_url,
                branch=branch,
                sub_path=sub_path,
                project_type=project_type,
                template=template,
                image_name=image_name,
                tag=tag,
                push=push,
                push_registry=push_registry,
                template_params=template_params or {},
                use_project_dockerfile=use_project_dockerfile,
                dockerfile_name=dockerfile_name,
                webhook_token=webhook_token,
                webhook_secret=webhook_secret,
                webhook_branch_filter=webhook_branch_filter,
                webhook_use_push_branch=webhook_use_push_branch,
                webhook_allowed_branches=webhook_allowed_branches or [],
                branch_tag_mapping=branch_tag_mapping or {},
                source_id=source_id,
                selected_services=selected_services or [],
                service_push_config=service_push_config or {},
                service_template_params=service_template_params or {},
                push_mode=push_mode or "multi",
                resource_package_configs=resource_package_configs or [],
                cron_expression=cron_expression,
                post_build_webhooks=post_build_webhooks or [],
                task_queue=[],
            )

            db.add(pipeline)
            db.commit()
            return pipeline_id
        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()

    def get_pipeline(self, pipeline_id: str) -> Optional[Dict]:
        """è·å–æµæ°´çº¿é…ç½®"""
        # ä½¿ç”¨åŸå§‹ SQL æŸ¥è¯¢æ¥é¿å… JSON è§£ç é—®é¢˜ï¼ˆä¸ list_pipelines ä¿æŒä¸€è‡´ï¼‰
        import sqlite3
        from backend.database import DB_FILE

        conn = sqlite3.connect(DB_FILE)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        try:
            # å…ˆå°è¯•å‚æ•°åŒ–æŸ¥è¯¢
            cursor.execute(
                "SELECT * FROM pipelines WHERE pipeline_id = ?", (pipeline_id,)
            )
            row = cursor.fetchone()

            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–æ‰€æœ‰æµæ°´çº¿å¹¶åœ¨å†…å­˜ä¸­åŒ¹é…ï¼ˆå¤„ç†å¯èƒ½çš„ç¼–ç é—®é¢˜ï¼‰
            if not row:
                cursor.execute("SELECT * FROM pipelines")
                all_rows = cursor.fetchall()
                for r in all_rows:
                    if r["pipeline_id"] == pipeline_id:
                        row = r
                        break

            if not row:
                return None

            # æ‰‹åŠ¨æ„å»ºå­—å…¸ï¼Œå®‰å…¨å¤„ç† JSON å­—æ®µï¼ˆä¸ list_pipelines é€»è¾‘ä¸€è‡´ï¼‰
            pipeline_dict = {
                "pipeline_id": row["pipeline_id"],
                "name": row["name"],
                "description": row["description"] or "",
                "enabled": bool(row["enabled"]),
                "git_url": row["git_url"],
                "branch": row["branch"],
                "sub_path": row["sub_path"],
                "project_type": row["project_type"],
                "template": row["template"],
                "image_name": row["image_name"],
                "tag": row["tag"],
                "push": bool(row["push"]),
                "push_registry": row["push_registry"],
                "template_params": self._safe_parse_json(row["template_params"], {}),
                "use_project_dockerfile": bool(row["use_project_dockerfile"]),
                "dockerfile_name": row["dockerfile_name"],
                "webhook_token": row["webhook_token"],
                "webhook_secret": row["webhook_secret"],
                "webhook_branch_filter": bool(row["webhook_branch_filter"]),
                "webhook_use_push_branch": bool(row["webhook_use_push_branch"]),
                "webhook_allowed_branches": self._safe_parse_json(
                    (
                        row["webhook_allowed_branches"]
                        if "webhook_allowed_branches" in row.keys()
                        else None
                    ),
                    [],
                ),
                "branch_tag_mapping": self._safe_parse_json(
                    row["branch_tag_mapping"], {}
                ),
                "source_id": row["source_id"],
                "selected_services": self._safe_parse_json(
                    row["selected_services"], []
                ),
                "service_push_config": self._safe_parse_json(
                    row["service_push_config"], {}
                ),
                "service_template_params": self._safe_parse_json(
                    row["service_template_params"], {}
                ),
                "push_mode": row["push_mode"],
                "resource_package_configs": self._safe_parse_json(
                    row["resource_package_configs"], []
                ),
                "cron_expression": row["cron_expression"],
                "next_run_time": row["next_run_time"],
                "post_build_webhooks": self._safe_parse_json(
                    (
                        row["post_build_webhooks"]
                        if "post_build_webhooks" in row.keys()
                        else None
                    ),
                    [],
                ),
                "current_task_id": row["current_task_id"],
                "task_queue": self._safe_parse_json(row["task_queue"], []),
                "created_at": row["created_at"],
                "updated_at": row["updated_at"],
                "last_triggered_at": row["last_triggered_at"],
                "trigger_count": row["trigger_count"] or 0,
            }

            # æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´
            from datetime import datetime

            for date_field in [
                "created_at",
                "updated_at",
                "last_triggered_at",
                "next_run_time",
            ]:
                if pipeline_dict[date_field]:
                    if isinstance(pipeline_dict[date_field], datetime):
                        pipeline_dict[date_field] = pipeline_dict[
                            date_field
                        ].isoformat()

            return pipeline_dict
        finally:
            conn.close()

    def get_pipeline_by_token(self, webhook_token: str) -> Optional[Dict]:
        """é€šè¿‡ Webhook Token è·å–æµæ°´çº¿é…ç½®"""
        db = get_db_session()
        try:
            pipeline = (
                db.query(Pipeline)
                .filter(Pipeline.webhook_token == webhook_token)
                .first()
            )
            return self._to_dict(pipeline)
        finally:
            db.close()

    def list_pipelines(self, enabled: bool = None) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰æµæ°´çº¿é…ç½®"""
        db = get_db_session()
        try:
            # ä½¿ç”¨åŸå§‹ SQL æŸ¥è¯¢æ¥é¿å… SQLAlchemy çš„ JSON è‡ªåŠ¨è§£ç é—®é¢˜
            import sqlite3
            from backend.database import DB_FILE

            conn = sqlite3.connect(DB_FILE)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            try:
                if enabled is not None:
                    cursor.execute(
                        "SELECT * FROM pipelines WHERE enabled = ? ORDER BY created_at DESC",
                        (enabled,),
                    )
                else:
                    cursor.execute("SELECT * FROM pipelines ORDER BY created_at DESC")

                rows = cursor.fetchall()
                result = []

                for row in rows:
                    try:
                        # æ‰‹åŠ¨æ„å»ºå­—å…¸ï¼Œå®‰å…¨å¤„ç† JSON å­—æ®µ
                        pipeline_dict = {
                            "pipeline_id": row["pipeline_id"],
                            "name": row["name"],
                            "description": row["description"] or "",
                            "enabled": bool(row["enabled"]),
                            "git_url": row["git_url"],
                            "branch": row["branch"],
                            "sub_path": row["sub_path"],
                            "project_type": row["project_type"],
                            "template": row["template"],
                            "image_name": row["image_name"],
                            "tag": row["tag"],
                            "push": bool(row["push"]),
                            "push_registry": row["push_registry"],
                            "template_params": self._safe_parse_json(
                                row["template_params"], {}
                            ),
                            "use_project_dockerfile": bool(
                                row["use_project_dockerfile"]
                            ),
                            "dockerfile_name": row["dockerfile_name"],
                            "webhook_token": row["webhook_token"],
                            "webhook_secret": row["webhook_secret"],
                            "webhook_branch_filter": bool(row["webhook_branch_filter"]),
                            "webhook_use_push_branch": bool(
                                row["webhook_use_push_branch"]
                            ),
                            "webhook_allowed_branches": self._safe_parse_json(
                                (
                                    row["webhook_allowed_branches"]
                                    if "webhook_allowed_branches" in row.keys()
                                    else None
                                ),
                                [],
                            ),
                            "branch_tag_mapping": self._safe_parse_json(
                                row["branch_tag_mapping"], {}
                            ),
                            "source_id": row["source_id"],
                            "selected_services": self._safe_parse_json(
                                row["selected_services"], []
                            ),
                            "service_push_config": self._safe_parse_json(
                                row["service_push_config"], {}
                            ),
                            "service_template_params": self._safe_parse_json(
                                row["service_template_params"], {}
                            ),
                            "push_mode": row["push_mode"],
                            "resource_package_configs": self._safe_parse_json(
                                row["resource_package_configs"], []
                            ),
                            "cron_expression": row["cron_expression"],
                            "next_run_time": row["next_run_time"],
                            "post_build_webhooks": self._safe_parse_json(
                                (
                                    row["post_build_webhooks"]
                                    if "post_build_webhooks" in row.keys()
                                    else None
                                ),
                                [],
                            ),
                            "current_task_id": row["current_task_id"],
                            "task_queue": self._safe_parse_json(row["task_queue"], []),
                            "created_at": row["created_at"],
                            "updated_at": row["updated_at"],
                            "last_triggered_at": row["last_triggered_at"],
                            "trigger_count": row["trigger_count"] or 0,
                        }

                        # æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´ï¼ˆSQLite è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è½¬æ¢ä¸º ISO æ ¼å¼ï¼‰
                        from datetime import datetime

                        for date_field in [
                            "created_at",
                            "updated_at",
                            "last_triggered_at",
                            "next_run_time",
                        ]:
                            if pipeline_dict[date_field]:
                                # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™è½¬æ¢ä¸º ISO æ ¼å¼
                                if isinstance(pipeline_dict[date_field], datetime):
                                    pipeline_dict[date_field] = pipeline_dict[
                                        date_field
                                    ].isoformat()
                                elif isinstance(pipeline_dict[date_field], str):
                                    # å·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œä¿æŒåŸæ ·
                                    pass

                        result.append(pipeline_dict)
                    except Exception as e:
                        pipeline_id = (
                            row["pipeline_id"]
                            if "pipeline_id" in row.keys()
                            else "Unknown"
                        )
                        print(f"âš ï¸ è·³è¿‡æµæ°´çº¿ {pipeline_id}: {e}")
                        import traceback

                        traceback.print_exc()

                return result
            finally:
                conn.close()
        finally:
            db.close()

    def _safe_parse_json(self, value, default):
        """å®‰å…¨åœ°è§£æ JSON å€¼"""
        if value is None or value == "":
            return default
        if isinstance(value, (dict, list)):
            return value
        try:
            import json

            return json.loads(value) if value else default
        except (json.JSONDecodeError, TypeError):
            return default

    def update_pipeline(
        self,
        pipeline_id: str,
        name: str = None,
        git_url: str = None,
        branch: str = None,
        project_type: str = None,
        template: str = None,
        image_name: str = None,
        tag: str = None,
        push: bool = None,
        push_registry: str = None,
        template_params: dict = None,
        sub_path: str = None,
        use_project_dockerfile: bool = None,
        dockerfile_name: str = None,
        webhook_secret: str = None,
        webhook_token: str = None,
        enabled: bool = None,
        description: str = None,
        cron_expression: str = None,
        webhook_branch_filter: bool = None,
        webhook_use_push_branch: bool = None,
        webhook_allowed_branches: list = None,
        branch_tag_mapping: dict = None,
        source_id: str = None,
        selected_services: list = None,
        service_push_config: dict = None,
        service_template_params: dict = None,
        push_mode: str = None,
        resource_package_configs: list = None,
        post_build_webhooks: list = None,
    ) -> bool:
        """æ›´æ–°æµæ°´çº¿é…ç½®"""
        db = get_db_session()
        try:
            # å…ˆåˆ·æ–° sessionï¼Œç¡®ä¿è·å–æœ€æ–°çš„æ•°æ®
            db.expire_all()

            pipeline = (
                db.query(Pipeline).filter(Pipeline.pipeline_id == pipeline_id).first()
            )
            if not pipeline:
                return False

            # åˆ·æ–°å¯¹è±¡ï¼Œç¡®ä¿ä¸æ•°æ®åº“åŒæ­¥
            db.refresh(pipeline)

            # æ›´æ–°å­—æ®µ
            if name is not None:
                pipeline.name = name
            if git_url is not None:
                pipeline.git_url = git_url
            if branch is not None:
                pipeline.branch = branch
            if project_type is not None:
                pipeline.project_type = project_type
            if template is not None:
                pipeline.template = template
            if image_name is not None:
                pipeline.image_name = image_name
            if tag is not None:
                pipeline.tag = tag
            if push is not None:
                pipeline.push = push
            if push_registry is not None:
                pipeline.push_registry = push_registry
            if template_params is not None:
                pipeline.template_params = template_params
            if sub_path is not None:
                pipeline.sub_path = sub_path
            if use_project_dockerfile is not None:
                pipeline.use_project_dockerfile = use_project_dockerfile
            if dockerfile_name is not None:
                pipeline.dockerfile_name = dockerfile_name
            if webhook_secret is not None:
                pipeline.webhook_secret = webhook_secret
            if webhook_token is not None:
                # æ£€æŸ¥ token æ˜¯å¦å·²è¢«å…¶ä»–æµæ°´çº¿ä½¿ç”¨
                existing = (
                    db.query(Pipeline)
                    .filter(
                        Pipeline.webhook_token == webhook_token,
                        Pipeline.pipeline_id != pipeline_id,
                    )
                    .first()
                )
                if existing:
                    raise ValueError(
                        f"Webhook Token '{webhook_token}' å·²è¢«å…¶ä»–æµæ°´çº¿ä½¿ç”¨"
                    )
                pipeline.webhook_token = webhook_token
            if enabled is not None:
                pipeline.enabled = enabled
            if description is not None:
                pipeline.description = description
            if cron_expression is not None:
                pipeline.cron_expression = cron_expression
            if webhook_branch_filter is not None:
                pipeline.webhook_branch_filter = webhook_branch_filter
            if webhook_use_push_branch is not None:
                pipeline.webhook_use_push_branch = webhook_use_push_branch
            if webhook_allowed_branches is not None:
                pipeline.webhook_allowed_branches = webhook_allowed_branches
            if branch_tag_mapping is not None:
                pipeline.branch_tag_mapping = branch_tag_mapping
            if source_id is not None:
                pipeline.source_id = source_id
            if selected_services is not None:
                pipeline.selected_services = selected_services
            if service_push_config is not None:
                pipeline.service_push_config = service_push_config
            if service_template_params is not None:
                pipeline.service_template_params = service_template_params
            if push_mode is not None:
                pipeline.push_mode = push_mode
            if resource_package_configs is not None:
                pipeline.resource_package_configs = resource_package_configs
            if post_build_webhooks is not None:
                pipeline.post_build_webhooks = post_build_webhooks

            pipeline.updated_at = datetime.now()

            # æäº¤æ›´æ”¹
            try:
                db.flush()  # å…ˆ flushï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                db.commit()
            except Exception as commit_error:
                # å¦‚æœ commit å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶å›æ»š
                db.rollback()
                error_msg = str(commit_error)
                print(f"âš ï¸ æ•°æ®åº“æäº¤å¤±è´¥: {error_msg}")
                print(f"âš ï¸ æµæ°´çº¿ID: {pipeline_id}")
                # å¦‚æœæ˜¯æ›´æ–°è¡Œæ•°ä¸åŒ¹é…çš„é”™è¯¯ï¼Œå¯èƒ½æ˜¯å¯¹è±¡å·²è¢«åˆ é™¤æˆ–å¹¶å‘ä¿®æ”¹
                if (
                    "expected to update" in error_msg.lower()
                    and "0 were matched" in error_msg.lower()
                ):
                    print(f"âš ï¸ æµæ°´çº¿å¯èƒ½åœ¨æ›´æ–°è¿‡ç¨‹ä¸­è¢«åˆ é™¤æˆ–å¹¶å‘ä¿®æ”¹: {pipeline_id}")
                    # é‡æ–°æŸ¥è¯¢ç¡®è®¤æµæ°´çº¿æ˜¯å¦è¿˜å­˜åœ¨
                    db.expire_all()  # æ¸…é™¤ç¼“å­˜
                    check_pipeline = (
                        db.query(Pipeline)
                        .filter(Pipeline.pipeline_id == pipeline_id)
                        .first()
                    )
                    if not check_pipeline:
                        print(f"âš ï¸ ç¡®è®¤ï¼šæµæ°´çº¿å·²è¢«åˆ é™¤: {pipeline_id}")
                        return False
                    else:
                        print(
                            f"âš ï¸ ç¡®è®¤ï¼šæµæ°´çº¿ä»ç„¶å­˜åœ¨ï¼Œå¯èƒ½æ˜¯å¹¶å‘ä¿®æ”¹å¯¼è‡´: {pipeline_id}"
                        )
                        # å°è¯•é‡æ–°æ›´æ–°ï¼šé‡æ–°è·å–å¯¹è±¡å¹¶æ›´æ–°
                        try:
                            db.refresh(check_pipeline)
                            # ä½¿ç”¨å­—å…¸æ”¶é›†éœ€è¦æ›´æ–°çš„å­—æ®µï¼Œç„¶åæ‰¹é‡åº”ç”¨
                            updates = {}
                            if name is not None:
                                updates["name"] = name
                            if git_url is not None:
                                updates["git_url"] = git_url
                            if branch is not None:
                                updates["branch"] = branch
                            if project_type is not None:
                                updates["project_type"] = project_type
                            if template is not None:
                                updates["template"] = template
                            if image_name is not None:
                                updates["image_name"] = image_name
                            if tag is not None:
                                updates["tag"] = tag
                            if push is not None:
                                updates["push"] = push
                            if push_registry is not None:
                                updates["push_registry"] = push_registry
                            if template_params is not None:
                                updates["template_params"] = template_params
                            if sub_path is not None:
                                updates["sub_path"] = sub_path
                            if use_project_dockerfile is not None:
                                updates["use_project_dockerfile"] = (
                                    use_project_dockerfile
                                )
                            if dockerfile_name is not None:
                                updates["dockerfile_name"] = dockerfile_name
                            if webhook_secret is not None:
                                updates["webhook_secret"] = webhook_secret
                            if webhook_token is not None:
                                # æ£€æŸ¥ token æ˜¯å¦å·²è¢«å…¶ä»–æµæ°´çº¿ä½¿ç”¨
                                existing = (
                                    db.query(Pipeline)
                                    .filter(
                                        Pipeline.webhook_token == webhook_token,
                                        Pipeline.pipeline_id != pipeline_id,
                                    )
                                    .first()
                                )
                                if existing:
                                    raise ValueError(
                                        f"Webhook Token '{webhook_token}' å·²è¢«å…¶ä»–æµæ°´çº¿ä½¿ç”¨"
                                    )
                                updates["webhook_token"] = webhook_token
                            if enabled is not None:
                                updates["enabled"] = enabled
                            if description is not None:
                                updates["description"] = description
                            if cron_expression is not None:
                                updates["cron_expression"] = cron_expression
                            if webhook_branch_filter is not None:
                                updates["webhook_branch_filter"] = webhook_branch_filter
                            if webhook_use_push_branch is not None:
                                updates["webhook_use_push_branch"] = (
                                    webhook_use_push_branch
                                )
                            if webhook_allowed_branches is not None:
                                updates["webhook_allowed_branches"] = (
                                    webhook_allowed_branches
                                )
                            if branch_tag_mapping is not None:
                                updates["branch_tag_mapping"] = branch_tag_mapping
                            if source_id is not None:
                                updates["source_id"] = source_id
                            if selected_services is not None:
                                updates["selected_services"] = selected_services
                            if service_push_config is not None:
                                updates["service_push_config"] = service_push_config
                            if service_template_params is not None:
                                updates["service_template_params"] = (
                                    service_template_params
                                )
                            if push_mode is not None:
                                updates["push_mode"] = push_mode
                            if resource_package_configs is not None:
                                updates["resource_package_configs"] = (
                                    resource_package_configs
                                )

                            # æ‰¹é‡åº”ç”¨æ›´æ–°
                            for key, value in updates.items():
                                setattr(check_pipeline, key, value)

                            check_pipeline.updated_at = datetime.now()
                            db.commit()
                            print(f"âœ… é‡è¯•æ›´æ–°æˆåŠŸ: {pipeline_id}")
                            return True
                        except Exception as retry_error:
                            db.rollback()
                            print(f"âš ï¸ é‡è¯•æ›´æ–°ä¹Ÿå¤±è´¥: {retry_error}")
                            raise ValueError(
                                f"æµæ°´çº¿æ›´æ–°å¤±è´¥ï¼Œå¯èƒ½æ˜¯å¹¶å‘ä¿®æ”¹: {error_msg}"
                            )
                raise

            return True
        except Exception as e:
            db.rollback()
            # å¦‚æœæ˜¯å­—æ®µä¸å­˜åœ¨çš„é”™è¯¯ï¼Œå°è¯•è¿ç§»æ•°æ®åº“
            if (
                "no such column" in str(e).lower()
                or "webhook_allowed_branches" in str(e).lower()
            ):
                try:
                    from backend.database import migrate_add_webhook_allowed_branches

                    migrate_add_webhook_allowed_branches()
                    # é‡è¯•æ›´æ–°
                    db.commit()
                    return True
                except Exception as migrate_error:
                    print(f"âš ï¸ æ•°æ®åº“è¿ç§»å¤±è´¥: {migrate_error}")
                    raise ValueError(f"æ•°æ®åº“å­—æ®µç¼ºå¤±ï¼Œè¯·é‡å¯åº”ç”¨ä»¥è‡ªåŠ¨è¿ç§»: {str(e)}")
            raise
        finally:
            db.close()

    def delete_pipeline(self, pipeline_id: str) -> bool:
        """åˆ é™¤æµæ°´çº¿é…ç½®"""
        db = get_db_session()
        try:
            pipeline = (
                db.query(Pipeline).filter(Pipeline.pipeline_id == pipeline_id).first()
            )
            if not pipeline:
                return False

            db.delete(pipeline)
            db.commit()
            return True
        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()

    def record_trigger(
        self,
        pipeline_id: str,
        task_id: str = None,
        trigger_source: str = "unknown",
        trigger_info: dict = None,
    ):
        """è®°å½•æµæ°´çº¿è§¦å‘"""
        db = get_db_session()
        try:
            pipeline = (
                db.query(Pipeline).filter(Pipeline.pipeline_id == pipeline_id).first()
            )
            if not pipeline:
                return

            pipeline.last_triggered_at = datetime.now()
            pipeline.trigger_count = (pipeline.trigger_count or 0) + 1

            if task_id:
                pipeline.current_task_id = task_id

                # è®°å½•åˆ°ä»»åŠ¡å†å²è¡¨
                history = PipelineTaskHistory(
                    pipeline_id=pipeline_id,
                    task_id=task_id,
                    trigger_source=trigger_source,
                    trigger_info=trigger_info or {},
                )
                db.add(history)
                # ç«‹å³åˆ·æ–°ï¼Œç¡®ä¿å¯¹è±¡å·²æŒä¹…åŒ–
                db.flush()

            # æäº¤ä¸»æ“ä½œ
            db.commit()

        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()

        # é™åˆ¶å†å²è®°å½•æ•°é‡ï¼ˆä¿ç•™æœ€è¿‘100æ¡ï¼‰- ä½¿ç”¨å•ç‹¬çš„æ•°æ®åº“ä¼šè¯
        if task_id:
            self._cleanup_old_history(pipeline_id)

    def _cleanup_old_history(self, pipeline_id: str):
        """æ¸…ç†æ—§çš„å†å²è®°å½•ï¼ˆä¿ç•™æœ€è¿‘100æ¡ï¼‰"""
        db = get_db_session()
        try:
            history_count = (
                db.query(PipelineTaskHistory)
                .filter(PipelineTaskHistory.pipeline_id == pipeline_id)
                .count()
            )
            if history_count > 100:
                # æŸ¥è¯¢éœ€è¦åˆ é™¤çš„æœ€æ—§è®°å½•
                oldest_records = (
                    db.query(PipelineTaskHistory)
                    .filter(PipelineTaskHistory.pipeline_id == pipeline_id)
                    .order_by(PipelineTaskHistory.triggered_at.asc())
                    .limit(history_count - 100)
                    .all()
                )
                # åˆ é™¤æœ€æ—§çš„è®°å½•
                for record in oldest_records:
                    db.delete(record)
                db.commit()
        except Exception as cleanup_error:
            # æ¸…ç†å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            print(f"âš ï¸ æ¸…ç†å†å²è®°å½•å¤±è´¥: {cleanup_error}")
            try:
                db.rollback()
            except:
                pass
        finally:
            db.close()

    def get_pipeline_running_task(self, pipeline_id: str) -> Optional[str]:
        """è·å–æµæ°´çº¿å½“å‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ID"""
        db = get_db_session()
        try:
            pipeline = (
                db.query(Pipeline).filter(Pipeline.pipeline_id == pipeline_id).first()
            )
            return pipeline.current_task_id if pipeline else None
        finally:
            db.close()

    def unbind_task(self, pipeline_id: str):
        """è§£ç»‘æµæ°´çº¿çš„ä»»åŠ¡ç»‘å®š"""
        db = get_db_session()
        try:
            pipeline = (
                db.query(Pipeline).filter(Pipeline.pipeline_id == pipeline_id).first()
            )
            if pipeline:
                pipeline.current_task_id = None
                db.commit()
        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()

    def add_task_to_queue(self, pipeline_id: str, task_config: dict) -> str:
        """å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—"""
        queue_id = str(uuid.uuid4())
        db = get_db_session()
        try:
            pipeline = (
                db.query(Pipeline).filter(Pipeline.pipeline_id == pipeline_id).first()
            )
            if not pipeline:
                raise ValueError(f"æµæ°´çº¿ä¸å­˜åœ¨: {pipeline_id}")

            queue = pipeline.task_queue or []
            queue.append(
                {
                    "queue_id": queue_id,
                    "task_config": task_config,
                    "created_at": datetime.now().isoformat(),
                }
            )
            pipeline.task_queue = queue
            pipeline.last_triggered_at = datetime.now()
            db.commit()
            return queue_id
        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()

    def get_queue_length(self, pipeline_id: str) -> int:
        """è·å–é˜Ÿåˆ—é•¿åº¦ï¼ˆä»å®é™…ä»»åŠ¡åˆ—è¡¨ä¸­ç»Ÿè®¡ï¼‰"""
        try:
            from backend.handlers import BuildManager

            build_manager = BuildManager()
            pending_tasks = build_manager.task_manager.list_tasks(status="pending")

            current_task_id = self.get_pipeline_running_task(pipeline_id)

            queue_count = 0
            for task in pending_tasks:
                task_config = task.get("task_config", {})
                task_pipeline_id = task_config.get("pipeline_id")
                task_id = task.get("task_id")

                if task_pipeline_id == pipeline_id and task_id != current_task_id:
                    queue_count += 1

            return queue_count
        except Exception as e:
            print(f"âš ï¸ è·å–é˜Ÿåˆ—é•¿åº¦å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            # å›é€€åˆ°ä½¿ç”¨å­—æ®µçš„æ–¹å¼
            db = get_db_session()
            try:
                pipeline = (
                    db.query(Pipeline)
                    .filter(Pipeline.pipeline_id == pipeline_id)
                    .first()
                )
                return len(pipeline.task_queue or []) if pipeline else 0
            finally:
                db.close()

    def get_next_queued_task(self, pipeline_id: str) -> Optional[dict]:
        """è·å–é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªä»»åŠ¡é…ç½®"""
        db = get_db_session()
        try:
            pipeline = (
                db.query(Pipeline).filter(Pipeline.pipeline_id == pipeline_id).first()
            )
            if pipeline and pipeline.task_queue:
                return pipeline.task_queue[0]
            return None
        finally:
            db.close()

    def remove_queued_task(self, pipeline_id: str, queue_id: str = None):
        """ä»é˜Ÿåˆ—ä¸­ç§»é™¤ä»»åŠ¡"""
        db = get_db_session()
        try:
            pipeline = (
                db.query(Pipeline).filter(Pipeline.pipeline_id == pipeline_id).first()
            )
            if not pipeline or not pipeline.task_queue:
                return

            queue = pipeline.task_queue
            if queue_id:
                pipeline.task_queue = [
                    q for q in queue if q.get("queue_id") != queue_id
                ]
            else:
                pipeline.task_queue = queue[1:] if len(queue) > 1 else []

            db.commit()
        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()

    def find_pipeline_by_task(self, task_id: str) -> Optional[str]:
        """æ ¹æ®ä»»åŠ¡IDæŸ¥æ‰¾ç»‘å®šçš„æµæ°´çº¿ID"""
        db = get_db_session()
        try:
            pipeline = (
                db.query(Pipeline).filter(Pipeline.current_task_id == task_id).first()
            )
            return pipeline.pipeline_id if pipeline else None
        finally:
            db.close()

    def check_debounce(self, pipeline_id: str, debounce_seconds: int = 3) -> bool:
        """æ£€æŸ¥æ˜¯å¦åœ¨é˜²æŠ–æ—¶é—´å†…"""
        db = get_db_session()
        try:
            pipeline = (
                db.query(Pipeline).filter(Pipeline.pipeline_id == pipeline_id).first()
            )
            if not pipeline or not pipeline.last_triggered_at:
                return False

            elapsed = (datetime.now() - pipeline.last_triggered_at).total_seconds()
            return elapsed < debounce_seconds
        finally:
            db.close()

    def _generate_config_hash(self, task_config: dict) -> str:
        """ç”Ÿæˆä»»åŠ¡é…ç½®çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºç›¸åŒä¿¡æ¯ï¼‰"""
        import hashlib
        import json

        # æå–å…³é”®å­—æ®µè¿›è¡Œæ¯”è¾ƒ
        key_fields = {
            "pipeline_id": task_config.get("pipeline_id"),
            "branch": task_config.get("branch"),
            "tag": task_config.get("tag"),
            "selected_services": (
                sorted(task_config.get("selected_services", []))
                if task_config.get("selected_services")
                else None
            ),
        }

        # ç”Ÿæˆå“ˆå¸Œå€¼
        config_str = json.dumps(key_fields, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(config_str.encode("utf-8")).hexdigest()

    def check_same_trigger_info(
        self, pipeline_id: str, task_config: dict, debounce_seconds: int = 3
    ) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºç›¸åŒä¿¡æ¯çš„è§¦å‘ï¼ˆç”¨äºé˜²æŠ–ï¼‰
        ä½¿ç”¨é¢„å æœºåˆ¶é¿å…å¹¶å‘ç«æ€æ¡ä»¶ï¼šå¦‚æœé€šè¿‡æ£€æŸ¥ï¼Œç«‹å³æ›´æ–°è®°å½•ï¼ˆé¢„å ï¼‰

        Args:
            pipeline_id: æµæ°´çº¿ID
            task_config: ä»»åŠ¡é…ç½®å­—å…¸
            debounce_seconds: é˜²æŠ–æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            True å¦‚æœæ˜¯ç›¸åŒä¿¡æ¯ä¸”åœ¨é˜²æŠ–æ—¶é—´å†…ï¼ŒFalse å¦åˆ™
        """
        with self.lock:
            # ç”Ÿæˆå½“å‰é…ç½®çš„å“ˆå¸Œå€¼
            current_hash = self._generate_config_hash(task_config)
            current_time = datetime.now()

            # è·å–æœ€åä¸€æ¬¡è§¦å‘çš„é…ç½®ä¿¡æ¯
            last_config = self._last_trigger_configs.get(pipeline_id)

            if not last_config:
                # æ²¡æœ‰å†å²è®°å½•ï¼Œä¸æ˜¯ç›¸åŒä¿¡æ¯ï¼Œç«‹å³é¢„å ï¼ˆæ›´æ–°è®°å½•ï¼‰
                print(
                    f"ğŸ” [é˜²æŠ–æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} æ²¡æœ‰å†å²è®°å½•ï¼Œé¢„å è®°å½•: hash={current_hash[:8]}..."
                )
                self._last_trigger_configs[pipeline_id] = {
                    "config_hash": current_hash,
                    "timestamp": current_time,
                }
                return False

            # æ£€æŸ¥æ—¶é—´é—´éš”
            elapsed = (current_time - last_config["timestamp"]).total_seconds()
            if elapsed >= debounce_seconds:
                # è¶…è¿‡é˜²æŠ–æ—¶é—´ï¼Œä¸æ˜¯ç›¸åŒä¿¡æ¯ï¼Œç«‹å³é¢„å ï¼ˆæ›´æ–°è®°å½•ï¼‰
                print(
                    f"ğŸ” [é˜²æŠ–æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} è¶…è¿‡é˜²æŠ–æ—¶é—´ ({elapsed:.2f}s >= {debounce_seconds}s)ï¼Œé¢„å è®°å½•: hash={current_hash[:8]}..."
                )
                self._last_trigger_configs[pipeline_id] = {
                    "config_hash": current_hash,
                    "timestamp": current_time,
                }
                return False

            # æ£€æŸ¥é…ç½®å“ˆå¸Œå€¼æ˜¯å¦ç›¸åŒ
            is_same = last_config["config_hash"] == current_hash
            if is_same:
                print(
                    f"ğŸš« [é˜²æŠ–æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} æ£€æµ‹åˆ°ç›¸åŒä¿¡æ¯ï¼ˆ{elapsed:.2f}så†…ï¼‰: hash={current_hash[:8]}..."
                )
            else:
                # é…ç½®ä¸åŒï¼Œä½†ä»åœ¨é˜²æŠ–æ—¶é—´å†…ï¼Œé¢„å è®°å½•ï¼ˆä½¿ç”¨æ–°çš„é…ç½®ï¼‰
                print(
                    f"ğŸ” [é˜²æŠ–æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} é…ç½®ä¸åŒä½†ä»åœ¨é˜²æŠ–æ—¶é—´å†…ï¼Œé¢„å æ–°è®°å½•: old_hash={last_config['config_hash'][:8]}..., new_hash={current_hash[:8]}..."
                )
                self._last_trigger_configs[pipeline_id] = {
                    "config_hash": current_hash,
                    "timestamp": current_time,
                }
            return is_same

    def update_last_trigger_config(self, pipeline_id: str, task_config: dict):
        """
        æ›´æ–°æœ€åä¸€æ¬¡è§¦å‘çš„é…ç½®ä¿¡æ¯

        Args:
            pipeline_id: æµæ°´çº¿ID
            task_config: ä»»åŠ¡é…ç½®å­—å…¸
        """
        with self.lock:
            config_hash = self._generate_config_hash(task_config)
            self._last_trigger_configs[pipeline_id] = {
                "config_hash": config_hash,
                "timestamp": datetime.now(),
            }

    def check_running_task_config(self, pipeline_id: str, task_config: dict) -> bool:
        """
        æ£€æŸ¥è¿è¡Œä¸­çš„ä»»åŠ¡æ˜¯å¦æ˜¯ç›¸åŒé…ç½®

        Args:
            pipeline_id: æµæ°´çº¿ID
            task_config: ä»»åŠ¡é…ç½®å­—å…¸

        Returns:
            True å¦‚æœè¿è¡Œä¸­çš„ä»»åŠ¡ä¹Ÿæ˜¯ç›¸åŒé…ç½®ï¼ŒFalse å¦åˆ™
        """
        try:
            # è·å–è¿è¡Œä¸­çš„ä»»åŠ¡ID
            current_task_id = self.get_pipeline_running_task(pipeline_id)
            if not current_task_id:
                print(f"ğŸ” [è¿è¡Œä¸­ä»»åŠ¡æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} æ²¡æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡")
                return False

            # è·å–ä»»åŠ¡ä¿¡æ¯
            from backend.handlers import BuildManager

            build_manager = BuildManager()
            task = build_manager.task_manager.get_task(current_task_id)
            if not task:
                print(
                    f"ğŸ” [è¿è¡Œä¸­ä»»åŠ¡æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} è¿è¡Œä¸­çš„ä»»åŠ¡ {current_task_id[:8]}... ä¸å­˜åœ¨"
                )
                return False

            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            task_status = task.get("status")
            if task_status not in ["pending", "running"]:
                print(
                    f"ğŸ” [è¿è¡Œä¸­ä»»åŠ¡æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} ä»»åŠ¡ {current_task_id[:8]}... çŠ¶æ€ä¸º {task_status}ï¼Œä¸åœ¨è¿è¡Œä¸­"
                )
                return False

            # è·å–ä»»åŠ¡é…ç½®
            task_config_data = task.get("task_config", {})
            if not task_config_data:
                print(
                    f"ğŸ” [è¿è¡Œä¸­ä»»åŠ¡æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} ä»»åŠ¡ {current_task_id[:8]}... æ²¡æœ‰é…ç½®ä¿¡æ¯"
                )
                return False

            # ç”Ÿæˆé…ç½®å“ˆå¸Œå€¼
            current_hash = self._generate_config_hash(task_config)
            task_hash = self._generate_config_hash(task_config_data)

            # æ¯”è¾ƒå“ˆå¸Œå€¼
            if task_hash == current_hash:
                print(
                    f"ğŸ” [è¿è¡Œä¸­ä»»åŠ¡æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} è¿è¡Œä¸­çš„ä»»åŠ¡ {current_task_id[:8]}... æ˜¯ç›¸åŒé…ç½®: hash={current_hash[:8]}..."
                )
                return True
            else:
                print(
                    f"ğŸ” [è¿è¡Œä¸­ä»»åŠ¡æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} è¿è¡Œä¸­çš„ä»»åŠ¡ {current_task_id[:8]}... é…ç½®ä¸åŒ: task_hash={task_hash[:8]}..., current_hash={current_hash[:8]}..."
                )
                return False
        except Exception as e:
            print(f"âš ï¸ [è¿è¡Œä¸­ä»»åŠ¡æ£€æŸ¥] æ£€æŸ¥è¿è¡Œä¸­ä»»åŠ¡å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            # å‡ºé”™æ—¶è¿”å› Falseï¼Œå…è®¸åˆ›å»ºä»»åŠ¡ï¼ˆé¿å…é˜»å¡ï¼‰
            return False

    def check_queued_task_exists(self, pipeline_id: str, task_config: dict) -> bool:
        """
        æ£€æŸ¥é˜Ÿåˆ—ä¸­æ˜¯å¦å·²æœ‰ç›¸åŒé…ç½®çš„å¾…æ‰§è¡Œä»»åŠ¡

        Args:
            pipeline_id: æµæ°´çº¿ID
            task_config: ä»»åŠ¡é…ç½®å­—å…¸

        Returns:
            True å¦‚æœé˜Ÿåˆ—ä¸­å·²æœ‰ç›¸åŒé…ç½®çš„ä»»åŠ¡ï¼ŒFalse å¦åˆ™
        """
        try:
            from backend.handlers import BuildManager

            build_manager = BuildManager()
            # è·å–æ‰€æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡ï¼ˆpending çŠ¶æ€ï¼‰
            pending_tasks = build_manager.task_manager.list_tasks(status="pending")

            # ç”Ÿæˆå½“å‰é…ç½®çš„å“ˆå¸Œå€¼
            current_hash = self._generate_config_hash(task_config)

            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒé…ç½®çš„ä»»åŠ¡
            for task in pending_tasks:
                task_config_data = task.get("task_config", {})
                task_pipeline_id = task_config_data.get("pipeline_id")

                # åªæ£€æŸ¥åŒä¸€æµæ°´çº¿çš„ä»»åŠ¡
                if task_pipeline_id == pipeline_id:
                    task_hash = self._generate_config_hash(task_config_data)
                    if task_hash == current_hash:
                        task_id = task.get("task_id", "unknown")
                        print(
                            f"ğŸ” [é˜Ÿåˆ—æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} é˜Ÿåˆ—ä¸­å·²å­˜åœ¨ç›¸åŒé…ç½®çš„ä»»åŠ¡: task_id={task_id[:8]}..., hash={current_hash[:8]}..."
                        )
                        return True

            print(
                f"ğŸ” [é˜Ÿåˆ—æ£€æŸ¥] æµæ°´çº¿ {pipeline_id[:8]} é˜Ÿåˆ—ä¸­æœªæ‰¾åˆ°ç›¸åŒé…ç½®çš„ä»»åŠ¡: hash={current_hash[:8]}..."
            )
            return False
        except Exception as e:
            print(f"âš ï¸ [é˜Ÿåˆ—æ£€æŸ¥] æ£€æŸ¥é˜Ÿåˆ—å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            # å‡ºé”™æ—¶è¿”å› Falseï¼Œå…è®¸åˆ›å»ºä»»åŠ¡ï¼ˆé¿å…é˜»å¡ï¼‰
            return False

    def check_duplicate_task(
        self, pipeline_id: str, task_config: dict, debounce_seconds: int = 3
    ) -> Optional[str]:
        """
        ç»¼åˆæ£€æŸ¥æ˜¯å¦æœ‰é‡å¤ä»»åŠ¡ï¼ˆé˜²æŠ–ã€è¿è¡Œä¸­ä»»åŠ¡ã€é˜Ÿåˆ—ï¼‰

        Args:
            pipeline_id: æµæ°´çº¿ID
            task_config: ä»»åŠ¡é…ç½®å­—å…¸
            debounce_seconds: é˜²æŠ–æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            "debounced" - é˜²æŠ–æ—¶é—´å†…çš„ç›¸åŒé…ç½®ï¼ˆç›´æ¥å±è”½ï¼‰
            "running_same_config" - è¿è¡Œä¸­çš„ä»»åŠ¡ä¹Ÿæ˜¯ç›¸åŒé…ç½®ï¼ˆéœ€è¦æ’é˜Ÿï¼‰
            "queued_same_config" - é˜Ÿåˆ—ä¸­å·²æœ‰ç›¸åŒé…ç½®çš„ä»»åŠ¡ï¼ˆéœ€è¦æ’é˜Ÿï¼‰
            None - æ²¡æœ‰é‡å¤ï¼Œå¯ä»¥åˆ›å»ºæ–°ä»»åŠ¡
        """
        print(
            f"ğŸ” [ç»¼åˆæ£€æŸ¥] å¼€å§‹æ£€æŸ¥é‡å¤ä»»åŠ¡: pipeline_id={pipeline_id[:8]}..., debounce_seconds={debounce_seconds}"
        )

        # 1. æ£€æŸ¥é˜²æŠ–æ—¶é—´å†…çš„ç›¸åŒé…ç½®
        is_same_trigger = self.check_same_trigger_info(
            pipeline_id, task_config, debounce_seconds
        )
        if is_same_trigger:
            print(
                f"ğŸš« [ç»¼åˆæ£€æŸ¥] é˜²æŠ–æ—¶é—´å†…çš„ç›¸åŒé…ç½®ï¼Œç›´æ¥å±è”½: pipeline_id={pipeline_id[:8]}..."
            )
            return "debounced"

        # 2. æ£€æŸ¥è¿è¡Œä¸­çš„ä»»åŠ¡æ˜¯å¦æ˜¯ç›¸åŒé…ç½®
        is_running_same_config = self.check_running_task_config(
            pipeline_id, task_config
        )
        if is_running_same_config:
            print(
                f"ğŸ” [ç»¼åˆæ£€æŸ¥] è¿è¡Œä¸­çš„ä»»åŠ¡ä¹Ÿæ˜¯ç›¸åŒé…ç½®ï¼Œéœ€è¦æ’é˜Ÿ: pipeline_id={pipeline_id[:8]}..."
            )
            return "running_same_config"

        # 3. æ£€æŸ¥é˜Ÿåˆ—ä¸­æ˜¯å¦æœ‰ç›¸åŒé…ç½®çš„ä»»åŠ¡
        is_queued_same_config = self.check_queued_task_exists(pipeline_id, task_config)
        if is_queued_same_config:
            print(
                f"ğŸ” [ç»¼åˆæ£€æŸ¥] é˜Ÿåˆ—ä¸­å·²æœ‰ç›¸åŒé…ç½®çš„ä»»åŠ¡ï¼Œéœ€è¦æ’é˜Ÿ: pipeline_id={pipeline_id[:8]}..."
            )
            return "queued_same_config"

        # 4. æ²¡æœ‰é‡å¤ï¼Œå¯ä»¥åˆ›å»ºæ–°ä»»åŠ¡
        print(
            f"âœ… [ç»¼åˆæ£€æŸ¥] æ²¡æœ‰é‡å¤ä»»åŠ¡ï¼Œå¯ä»¥åˆ›å»ºæ–°ä»»åŠ¡: pipeline_id={pipeline_id[:8]}..."
        )
        return None

    def verify_webhook_signature(
        self,
        payload: bytes,
        signature: str,
        secret: str,
        signature_header: str = "sha256",
    ) -> bool:
        """éªŒè¯ Webhook ç­¾å"""
        try:
            if "=" in signature:
                algo, sig = signature.split("=", 1)
            else:
                algo = signature_header
                sig = signature

            if algo.lower() == "sha1":
                mac = hmac.new(secret.encode(), payload, hashlib.sha1)
            elif algo.lower() == "sha256":
                mac = hmac.new(secret.encode(), payload, hashlib.sha256)
            else:
                print(f"âŒ ä¸æ”¯æŒçš„ç­¾åç®—æ³•: {algo}")
                return False

            expected_sig = mac.hexdigest()
            result = hmac.compare_digest(expected_sig, sig)

            if not result:
                print(
                    f"âŒ ç­¾åä¸åŒ¹é…: expected={expected_sig[:8]}..., got={sig[:8]}..., algo={algo}"
                )

            return result
        except Exception as e:
            print(f"âŒ Webhook ç­¾åéªŒè¯å¼‚å¸¸: {e}")
            import traceback

            traceback.print_exc()
            return False
